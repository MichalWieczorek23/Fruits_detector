from pathlib import Path

import cv2
import numpy as np
import time

from src.models import EdgeBoxes

from keras._tf_keras.keras.applications import EfficientNetB0
from keras._tf_keras.keras.models import Model
from keras._tf_keras.keras.preprocessing.image import img_to_array

import pickle

#### For test purposes
# parent_path = Path(__file__).resolve().parent.parent.parent
# img = Image.open(parent_path / "src" / "venice.jpg")
# img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
#
# cv2.imshow("Image in OpenCV", img_cv)
# cv2.waitKey(0)
# cv2.destroyAllWindows()
#### For test purposes
# start = time.time()
# boxes = EdgeBoxes.EdgeBoxes(image, False)
# end = time.time()
#
# print(end - start)
#### For test purposes
# Function to draw bounding boxes
# def draw_bboxes(image, bboxes):
#     for bbox in bboxes:
#         bbox = [int(x) for x in bbox]
#         x, y, w, h = bbox
#         image = cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)
#     return image

#### For test purposes
# parent_path = Path(__file__).resolve().parent.parent.parent
# for image_info in images:
#     # Path to file
#     img = Image.open( parent_path / "data" / "images" / "train" / str(image_info['file_name']) )
#
#     # Conversion to CV2 format
#     img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
#
#     # Getting the coordinates of the objects (assuming you have a bbox list)
#     annotations = []  # Informations about the object
#     for ann in data['annotations']:
#         if ann['image_id'] == image_info['id']:
#             annotations.append(ann['bbox'])  # Assuming that 'bbox' is a list of coordinates like: [x, y, w, h]
#
#     # Drawing bounding boxes
#     image_with_bboxes = draw_bboxes(img, annotations)
#
#     # Display images with bounding boxes
#     cv2.imshow(f"Image {image_info['file_name']}", image_with_bboxes)
#     cv2.waitKey(0)
#     cv2.destroyAllWindows()

class Rcnn():
    def __init__(self):
        conv_base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
        self.feature_extractor = Model(inputs=conv_base.input, outputs=conv_base.layers[-1].output)
        parent_path = Path(__file__).resolve().parent.parent.parent
        models_path = parent_path / "models" / "svm"
        with open(models_path / "Apple" / "svc_for_apple_acc_0.919_len_10000.pkl", "rb") as file:
            self.svc_apple_classifier = pickle.load(file)
            # print("Model type:", type(self.svc_model))
        with open(models_path / "Banana" / "svc_for_banana_acc_0.943_len_10000.pkl", "rb") as file:
            self.svc_banana_classifier = pickle.load(file)
        with open(models_path / "Carrot" / "svc_for_carrot_acc_0.9315_len_10000.pkl", "rb") as file:
            self.svc_carrot_classifier = pickle.load(file)
        with open(models_path / "Orange" / "svc_for_orange_acc_0.95.pkl", "rb") as file:
            self.svc_orange_classifier = pickle.load(file)

    def search_image(self, img):
        pred = 0
        # Generating ROIs
        img, boxes = EdgeBoxes.EdgeBoxes(img)

        start = time.time()
        # In a loop iteration goes through already generated ROIs
        for j, (x, y, w, h) in enumerate(boxes):
            # If ROI is too small, there is no need to check it
            if w * h < 0.3 * 224 * 224:
                continue

            # From the main image, an area is cut out according to the coordinates generated by the EdgeBoxes algorithm
            cropped_region = img[y:y + h, x:x + w]
            resized_img = np.resize(cropped_region, (224, 224, 3))
            input_arr = np.expand_dims(img_to_array(resized_img), axis=0)

            # Feature extraction
            features = self.feature_extractor.predict(input_arr)
            flattened_features = features.reshape(features.shape[0], features.shape[1] * features.shape[2] * features.shape[3])

            # Classification
            predictions_apple = self.svc_apple_classifier.predict(flattened_features)
            predictions_banana = self.svc_banana_classifier.predict(flattened_features)
            predictions_carrot = self.svc_carrot_classifier.predict(flattened_features)
            predictions_orange = self.svc_orange_classifier.predict(flattened_features)
            print(f"Apple predictions: {predictions_apple}")
            print(f"Banana predictions: {predictions_banana}")
            print(f"Orange predictions: {predictions_carrot}")
            print(f"Carrot predictions: {predictions_orange}")


            if predictions_apple > 0.5:
                cv2.rectangle(img, (x, y), (x + w, y + h), (79, 168, 69), 2)
            if predictions_banana > 0.5:
                cv2.rectangle(img, (x, y), (x + w, y + h), (66, 235, 218), 2)
            if predictions_carrot > 0.5:
                cv2.rectangle(img, (x, y), (x + w, y + h), (88, 150, 209), 2)
            if predictions_orange > 0.5:
                cv2.rectangle(img, (x, y), (x + w, y + h), (54, 131, 179), 2)

            # Instruction for debug purposes:
            # else:
            #     cv2.rectangle(img, (x, y), (x + w, y + h), (105, 100, 96), 2)
            # cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

        end = time.time()
        print(f"Time{end - start}")
        return img

rcnn = Rcnn()